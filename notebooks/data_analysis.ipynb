{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e6cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Raw Data Preview:\n",
      "+--------+-----+\n",
      "|Category|Value|\n",
      "+--------+-----+\n",
      "|       A|   10|\n",
      "|       B|   15|\n",
      "|       A|   20|\n",
      "|       C|    5|\n",
      "|       B|   25|\n",
      "+--------+-----+\n",
      "\n",
      "Aggregated Results:\n",
      "+--------+------------+-----+----------+\n",
      "|Category|AverageValue|Count|TotalValue|\n",
      "+--------+------------+-----+----------+\n",
      "|       C|         5.0|    1|       5.0|\n",
      "|       B|        20.0|    2|      40.0|\n",
      "|       A|        15.0|    2|      30.0|\n",
      "+--------+------------+-----+----------+\n",
      "\n",
      "+--------+-----+\n",
      "|Category|Value|\n",
      "+--------+-----+\n",
      "|       A| 10.0|\n",
      "|       B| 15.0|\n",
      "|       A| 20.0|\n",
      "|       B| 25.0|\n",
      "+--------+-----+\n",
      "\n",
      "‚úÖ Aggregated data written to CSV: data/output\\aggregated_data.csv\n"
     ]
    }
   ],
   "source": [
    "# PySpark in Microsoft Fabric Notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, desc, sum as spark_sum\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create Spark session (Fabric usually provides this automatically)\n",
    "spark = SparkSession.builder.appName(\"FabricPySparkAnalysis\").getOrCreate()\n",
    "\n",
    "# Path to CSV file in Fabric Lakehouse or local path\n",
    "# In Fabric, replace with: \"Files/sample_data.csv\" or lakehouse path\n",
    "\n",
    "# Use Fabric path if available, else local path\n",
    "if os.path.exists(\"Files/sample_data.csv\"):\n",
    "    data_path = \"Files/sample_data.csv\"  # Fabric Lakehouse\n",
    "else:\n",
    "    data_path = \"data/sample_data.csv\"   # Local\n",
    "\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(data_path)\n",
    "try:\n",
    "    # Load CSV with header and schema inference\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(data_path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading data: {e}\")\n",
    "\n",
    "# Show first rows\n",
    "print(\"üìÑ Raw Data Preview:\")\n",
    "df.show(5)\n",
    "\n",
    "# Basic data cleaning: remove nulls in important columns\n",
    "df_clean = df.dropna(subset=[\"Category\", \"Value\"])\n",
    "# Cast Value to numeric to avoid mixed-type aggregation errors\n",
    "df_clean = df_clean.withColumn(\"Value\", col(\"Value\").cast(\"double\"))\n",
    "# Drop rows where cast produced nulls (non-numeric values)\n",
    "df_clean = df_clean.na.drop(subset=[\"Value\"])\n",
    "\n",
    "# Example aggregation: average value per category\n",
    "agg_df = (\n",
    "    df_clean.groupBy(\"Category\")\n",
    "    .agg(avg(\"Value\").alias(\"AverageValue\"), count(\"*\").alias(\"Count\"), spark_sum(\"Value\").alias(\"TotalValue\"))\n",
    "    .orderBy(desc(\"Category\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"Aggregated Results:\")\n",
    "agg_df.show()\n",
    "\n",
    "\n",
    "df_clean.where(col(\"Value\") > 5).show() \n",
    "\n",
    "output_path = \"data/output\"\n",
    "\n",
    "# Ensure parent directory exists for local runs (Fabric 'Files/' path handled by service)\n",
    "parent = os.path.dirname(output_path)\n",
    "if parent:\n",
    "    os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "# Write as CSV using Pandas (no Hadoop libraries needed).\n",
    "# In Fabric, you can use Spark's write.parquet() directly instead.\n",
    "try:\n",
    "    pandas_df = agg_df.toPandas()\n",
    "    csv_file = os.path.join(output_path, \"aggregated_data.csv\")\n",
    "    pandas_df.to_csv(csv_file, index=False)\n",
    "    print(f\"‚úÖ Aggregated data written to CSV: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing CSV to {output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e27fe",
   "metadata": {},
   "source": [
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit: Fabric + PySpark project\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/hansenguxd/fabric-pyspark-analysis.git\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddeb6b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
