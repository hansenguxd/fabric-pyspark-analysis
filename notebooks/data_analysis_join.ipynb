{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Raw Data Preview:\n",
      "+--------+-----+\n",
      "|Category|Value|\n",
      "+--------+-----+\n",
      "|       A|   10|\n",
      "|       B|   15|\n",
      "|       A|   20|\n",
      "|       C|    5|\n",
      "|       B|   25|\n",
      "+--------+-----+\n",
      "\n",
      "Aggregated Results:\n",
      "+--------+------------+-----+----------+\n",
      "|Category|AverageValue|Count|TotalValue|\n",
      "+--------+------------+-----+----------+\n",
      "|       C|         5.0|    1|       5.0|\n",
      "|       B|        20.0|    2|      40.0|\n",
      "|       A|        15.0|    2|      30.0|\n",
      "+--------+------------+-----+----------+\n",
      "\n",
      "+--------+-----+\n",
      "|Category|Value|\n",
      "+--------+-----+\n",
      "|       A| 10.0|\n",
      "|       B| 15.0|\n",
      "|       A| 20.0|\n",
      "|       B| 25.0|\n",
      "+--------+-----+\n",
      "\n",
      "‚úÖ Aggregated data written to CSV: data/output\\aggregated_data.csv\n"
     ]
    }
   ],
   "source": [
    "# PySpark in Microsoft Fabric Notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, desc, sum as spark_sum\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create Spark session (Fabric usually provides this automatically)\n",
    "spark = SparkSession.builder.appName(\"FabricPySparkAnalysis\").getOrCreate()\n",
    "\n",
    "# Path to CSV file in Fabric Lakehouse or local path\n",
    "# In Fabric, replace with: \"Files/sample_data.csv\" or lakehouse path\n",
    "\n",
    "# Use Fabric path if available, else local path\n",
    "if os.path.exists(\"Files/sample_data.csv\"):\n",
    "    data_path = \"Files/sample_data.csv\"  # Fabric Lakehouse\n",
    "else:\n",
    "    policy_path = \"data/policy_data.csv\"   # Local\n",
    "    plan_path = \"data/plan_data.csv\"       # Local\n",
    "    client_path = \"data/client_data.csv\"   # Local    \n",
    "\n",
    "df_policy = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(policy_path)\n",
    "df_plan = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(plan_path)\n",
    "df_client = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(client_path)\n",
    "\n",
    "\n",
    "# Show first rows\n",
    "print(\"üìÑ Raw Data Preview:\")\n",
    "df_policy.show(5)\n",
    "df_plan.show(5)\n",
    "df_client.show(5)\n",
    "\n",
    "\n",
    "# Cast Value to numeric to avoid mixed-type aggregation errors\n",
    "df_policy = df_policy.dropna(subset=[\"policy_id\", \"plan_id\",\"client_id\", \"coverage_amount\"])\n",
    "df_policy = df_policy.withColumn(\"coverage_amount\", col(\"coverage_amount\").cast(\"double\"))\n",
    "\n",
    "df_plan=df_plan.dropna(subset=[\"plan_id\", \"plan_name\",\"premium_amount\"])\n",
    "df_plan = df_plan.withColumn(\"premium_amount\", col(\"premium_amount\").cast(\"double\"))\n",
    "\n",
    "df_client=df_client.dropna(subset=[\"client_id\", \"client_name\",\"client_address\"])\n",
    "\n",
    "\n",
    "# Join dataframes\n",
    "df_joined = df_policy.join(df_plan, on=\"plan_id\", how=\"inner\")\n",
    "df_joined = df_joined.join(df_client, on=\"client_id\", how=\"inner\").select(\"policy_id\", \"client_name\", \"client_address\", \"plan_name\", \"coverage_amount\", \"premium_amount\")\n",
    "\n",
    "df_joined.show()\n",
    "\n",
    "# Example aggregation: average value per category\n",
    "agg_df = (\n",
    "    df_joined.groupBy(\"plan_name\")\n",
    "    .agg(avg(\"coverage_amount\").alias(\"AverageCoverage\"), count(\"*\").alias(\"Count\"), spark_sum(\"premium_amount\").alias(\"TotalPremium\"))\n",
    "    .orderBy(desc(\"TotalPremium\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"Aggregated Results:\")\n",
    "agg_df.show()\n",
    "\n",
    "\n",
    "output_path = \"data/output\"\n",
    "\n",
    "# Ensure parent directory exists for local runs (Fabric 'Files/' path handled by service)\n",
    "parent = os.path.dirname(output_path)\n",
    "if parent:\n",
    "    os.makedirs(parent, exist_ok=True)\n",
    "\n",
    "# Write as CSV using Pandas (no Hadoop libraries needed).\n",
    "# In Fabric, you can use Spark's write.parquet() directly instead.\n",
    "try:\n",
    "    pandas_df = agg_df.toPandas()\n",
    "    csv_file = os.path.join(output_path, \"aggregated_data.csv\")\n",
    "    pandas_df.to_csv(csv_file, index=False)\n",
    "    print(f\"‚úÖ Aggregated data written to CSV: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing CSV to {output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e27fe",
   "metadata": {},
   "source": [
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit: Fabric + PySpark project\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/hansenguxd/fabric-pyspark-analysis.git\n",
    "git push -u origin main\n",
    "\n",
    "git status\n",
    "# push one file\n",
    "git add notebooks/data_analysis.ipynb\n",
    "git commit -m \"Improved aggregation logic\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddeb6b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
